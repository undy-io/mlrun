apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}
  namespace: {{ .Values.namespace }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app: {{ .Release.Name }}
        borg/expose: {{ .Values.borg.expose }}
      annotations:
        borg/models: {{ .Values.model.name }}
    spec:
      containers:
        - name: vllm
          image: "{{ .Values.imageName }}:{{ .Values.imageVersion }}"
          args:
            - "python3"
            - "-m"
            - "vllm.entrypoints.openai.api_server"
            - "--model={{ .Values.model.path }}"
            {{- if .Values.model.name }}
            - "--model-name={{ .Values.model.name }}"
            {{- end }}
            {{- if gt (int .Values.gpu.count) 1 }}
            - "--tensor-parallel-size={{ .Values.gpu.count }}" 
            {{- end }}
            {{- if .Values.model.args }}
            {{- $json := .Values.model.args| b64dec }}
            {{- $argsList := $json | fromJsonArray }}
            {{- range $argsList }}
            - {{ . | quote }}
            {{- end }}
            {{- end }}
          env:
            {{- if .Values.model.offline }}
            - name: HF_HUB_OFFLINE
              value: 1
            {{- end }}
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
          resources:
            limits:
              cpu: {{ .Values.resources.limits.cpu }}
              memory: {{ .Values.resources.limits.memory }}
              {{- if gt (int .Values.gpu.count) 0 }}
              {{ .Values.gpu.type }}: {{ .Values.gpu.count }}
              {{- end }}
            requests:
              cpu: {{ .Values.resources.requests.cpu }}
              memory: {{ .Values.resources.requests.memory }}
              {{- if gt (int .Values.gpu.count) 0 }}
              {{ .Values.gpu.type }}: {{ .Values.gpu.count }}
              {{- end }}
          volumeMounts:
            - name: shm
              mountPath: /dev/shm
            - name: huggingface-cache
              mountPath: {{ .Values.model.mountPath }}
              readOnly: {{ .Values.model.readonly }}
      volumes:
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: "{{ .Values.sharedMemory }}"
        - name: huggingface-cache
          hostPath:
            path: /srv/huggingface
            type: DirectoryOrCreate
      {{- if gt (int .Values.gpu.count) 0 }}
      tolerations:
        - key: {{ .Values.gpu.type }}
          operator: Exists
      {{- end }}